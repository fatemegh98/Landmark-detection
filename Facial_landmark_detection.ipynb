{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj9YcAnsT4B_"
      },
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4qO3Q2b07ni",
        "outputId": "7374d8c1-f652-48a9-83a1-dbaf1ac67c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget   http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 # DOWNLOAD LINK\n",
        "\n",
        "!bunzip2 /content/shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "datFile =  \"/content/shape_predictor_68_face_landmarks.dat\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBeF2pCtI-25",
        "outputId": "110b0906-467e-4f65-f301-75bf2e376769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-25 12:30:24--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  15.1MB/s    in 5.2s    \n",
            "\n",
            "2024-01-25 12:30:30 (11.7 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path to the zip file on Google Drive\n",
        "zip_file_path = \"/content/drive/MyDrive/Datas.zip\"\n",
        "\n",
        "# Specify the target folder for extraction in Colab\n",
        "extracted_folder = \"Ims\"\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "os.makedirs(extracted_folder, exist_ok=True)\n",
        "\n",
        "# Copy the zip file from Google Drive to Colab\n",
        "!cp \"{zip_file_path}\" .\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Datas.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "print(f\"File '{zip_file_path}' has been extracted to '{extracted_folder}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4S0vuZWMVU_",
        "outputId": "cab02829-ddbe-4851-fb03-a9689009f7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "File '/content/drive/MyDrive/Datas.zip' has been extracted to 'Ims'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Draw bounding boxes and keypoints on an image\n",
        "def draw_facebox_and_keypoints(filename, result_list, save_path=None):\n",
        "    # Load the image\n",
        "    data = plt.imread(filename)\n",
        "    # Plot the image\n",
        "    plt.imshow(data)\n",
        "    # Get the context for drawing boxes\n",
        "    ax = plt.gca()\n",
        "    # Plot each box\n",
        "    for result in result_list:\n",
        "        # Check if 'box' is present in the result dictionary\n",
        "        if 'box' in result:\n",
        "            # Get coordinates\n",
        "            x, y, width, height = result['box']\n",
        "            # Create the shape\n",
        "            rect = plt.Rectangle((x, y), width, height, fill=False, color='orange')\n",
        "            # Draw the box\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        # Check if 'keypoints' is present in the result dictionary\n",
        "        if 'keypoints' in result:\n",
        "            # Draw the dots\n",
        "            for key, value in result['keypoints'].items():\n",
        "                # Create and draw dot\n",
        "                dot = plt.Circle(value, radius=2, color='red')\n",
        "                ax.add_patch(dot)\n",
        "\n",
        "    # Save or display the plot\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "LOlh-C8LX-89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize face detector and shape predictor\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(datFile)  # You need to download this file\n",
        "\n",
        "# Directory containing your images\n",
        "images_directory = '/content/Ims/Camera Roll'\n",
        "# Output directory to save resized images\n",
        "resized_directory = 'resized_images'\n",
        "os.makedirs(resized_directory, exist_ok=True)\n",
        "target_height, target_width, channels = 256, 256, 3\n",
        "for filename in os.listdir(images_directory):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "      original_image_path = os.path.join(images_directory, filename)\n",
        "\n",
        "        # Read the original image\n",
        "      image = cv2.imread(original_image_path)\n",
        "      resized_image = cv2.resize(image, (target_width, target_height))\n",
        "      resized_image = np.array(resized_image)\n",
        "      resized_path = os.path.join(resized_directory, f'resized_{filename}')\n",
        "      cv2.imwrite(resized_path, resized_image)"
      ],
      "metadata": {
        "id": "XgevEmRgvr3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_directory = '/content/resized_images'\n",
        "\n",
        "# Output directory to save annotated images\n",
        "output_directory = 'annotated_images'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Lists to store image paths and corresponding landmarks\n",
        "image_paths = []\n",
        "landmarks_list = []\n",
        "\n",
        "\n",
        "# Iterate through images in the directory\n",
        "for filename in os.listdir(images_directory):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        # Load the image\n",
        "        image_path = os.path.join(images_directory, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "\n",
        "\n",
        "        # Detect faces in the image\n",
        "        faces = detector(image)\n",
        "\n",
        "        # Loop through detected faces\n",
        "        for face_index, face in enumerate(faces):\n",
        "            # Get bounding box coordinates\n",
        "            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
        "\n",
        "            # Get facial landmarks\n",
        "            landmarks = predictor(image, face)\n",
        "            keypoints = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)]  # Assuming 68 landmarks\n",
        "\n",
        "            # Save or display the annotated image for each face\n",
        "            output_path = os.path.join(output_directory, f'annotated_{filename[:-4]}_face{face_index}.png')\n",
        "            draw_facebox_and_keypoints(image_path, [{'box': (x, y, w, h), 'keypoints': dict(enumerate(keypoints))}], save_path=output_path)\n",
        "\n",
        "            # Append image path and landmarks to the lists\n",
        "            image_paths.append(image_path)\n",
        "            landmarks_list.append(keypoints)\n",
        "\n",
        "# Create a DataFrame to store image paths and landmarks\n",
        "df = pd.DataFrame({'image_path': image_paths, 'landmarks': landmarks_list})\n",
        "\n",
        "# Save the DataFrame to a CSV file (adjust the filename)\n",
        "df.to_csv('landmarks_annotations.csv', index=False)\n",
        "\n",
        "print(\"Annotation of images completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JervIdIut-E",
        "outputId": "b4675677-54f1-49e2-d9e5-9a142013de3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotation of images completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, PReLU, MaxPooling2D, Flatten, Dense, Softmax, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import Sequence\n",
        "\n",
        "# Load the CSV file containing image paths and landmarks\n",
        "annotations_file = 'landmarks_annotations.csv'\n",
        "annotations_df = pd.read_csv(annotations_file)\n",
        "\n",
        "# Assuming landmarks are stored as a string in the DataFrame; convert them to numpy arrays\n",
        "annotations_df['landmarks'] = annotations_df['landmarks'].apply(eval)\n",
        "\n",
        "# Extract image paths and landmarks\n",
        "image_paths = annotations_df['image_path'].values\n",
        "landmarks = np.array(list(annotations_df['landmarks'].values))\n",
        "\n",
        "# Load and preprocess images\n",
        "images = []\n",
        "import cv2\n",
        "\n",
        "# # Assuming you have a target height, width, and channels\n",
        "# target_height, target_width, channels = 128, 128, 3\n",
        "\n",
        "for image_path in image_paths:\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "#     # Resize the image to the target size\n",
        "#     image = cv2.resize(image, (target_width, target_height))\n",
        "\n",
        "    # Normalize pixel values to the range [0, 1]\n",
        "    image = image / 255.0\n",
        "\n",
        "#     # Add the preprocessed image to the list\n",
        "    images.append(image)\n",
        "\n",
        "# # Convert the list of images to a numpy array\n",
        "# images = np.array(images)\n",
        "\n",
        "# Flatten landmarks to match the output shape of the model\n",
        "landmarks = landmarks.reshape((landmarks.shape[0], -1))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, landmarks, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Create an instance of the model\n",
        "def LandmarkDetector(input_shape=(256, 256, 3)):\n",
        "    input_tensor = Input(shape=input_shape, name='data')\n",
        "\n",
        "    x = Conv2D(32, (3, 3), strides=(1, 1), padding='valid', activation=None, name='conv1')(input_tensor)\n",
        "    x = PReLU(name='prelu1')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pool1')(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='valid', activation=None, name='conv2')(x)\n",
        "    x = PReLU(name='prelu2')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pool2')(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='valid', activation=None, name='conv3')(x)\n",
        "    x = PReLU(name='prelu3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(x)\n",
        "\n",
        "    x = Conv2D(128, (2, 2), strides=(1, 1), padding='valid', activation=None, name='conv4')(x)\n",
        "    x = PReLU(name='prelu4')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(256, activation=None, name='conv5')(x)\n",
        "    x = PReLU(name='prelu5')(x)\n",
        "\n",
        "    # Adding dropout for regularization\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    landmarks = Dense(136, activation=None, name='landmarks')(x)  # 68 points * 2 coordinates\n",
        "\n",
        "    output1 = Softmax(name='prob1')(landmarks)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=[output1, landmarks])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create an instance of the model\n",
        "landmark_detector_model = LandmarkDetector()\n",
        "\n",
        "# Compile the model\n",
        "landmark_detector_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "landmark_detector_model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Create a custom data generator\n",
        "class LandmarkDataGenerator(Sequence):\n",
        "    def __init__(self, X, y, batch_size, datagen):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.datagen = datagen\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start = index * self.batch_size\n",
        "        end = (index + 1) * self.batch_size\n",
        "\n",
        "        batch_X = self.X[start:end]\n",
        "        batch_y = self.y[start:end]\n",
        "\n",
        "        # Apply data augmentation\n",
        "        augmented_images, augmented_landmarks = [], []\n",
        "        for i in range(len(batch_X)):\n",
        "            image = batch_X[i]\n",
        "            landmark = batch_y[i]\n",
        "\n",
        "            augmented_image, _ = self.datagen.flow(np.expand_dims(image, axis=0), np.zeros(1), batch_size=1).next()\n",
        "            augmented_landmark = landmark  # No augmentation for landmarks, adjust as needed\n",
        "\n",
        "            augmented_images.append(augmented_image.squeeze())\n",
        "            augmented_landmarks.append(augmented_landmark)\n",
        "\n",
        "        return np.array(augmented_images), [np.array(augmented_landmarks), np.array(augmented_landmarks)]\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create custom data generators\n",
        "batch_size = 16  # Adjust the batch size as needed\n",
        "\n",
        "train_data_generator = LandmarkDataGenerator(X_train, y_train, batch_size=batch_size, datagen=datagen)\n",
        "val_data_generator = LandmarkDataGenerator(X_val, y_val, batch_size=batch_size, datagen=datagen)\n",
        "\n",
        "# Calculate steps per epoch and validation steps\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "validation_steps = len(X_val) // batch_size\n",
        "\n",
        "# If the number of samples is not perfectly divisible by the batch size, you can add an additional step\n",
        "if len(X_train) % batch_size != 0:\n",
        "    steps_per_epoch += 1\n",
        "\n",
        "if len(X_val) % batch_size != 0:\n",
        "    validation_steps += 1\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = landmark_detector_model.fit_generator(\n",
        "    train_data_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_data_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "loss = landmark_detector_model.evaluate_generator(val_data_generator, steps=validation_steps)\n",
        "print(f\"Validation Loss: {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBQKWOJ8OVhp",
        "outputId": "ff6c29e8-e628-4dc3-9312-a434ab5b611d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " data (InputLayer)           [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " conv1 (Conv2D)              (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " prelu1 (PReLU)              (None, 254, 254, 32)      2064512   \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 126, 126, 32)      0         \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (None, 124, 124, 64)      18496     \n",
            "                                                                 \n",
            " prelu2 (PReLU)              (None, 124, 124, 64)      984064    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 61, 61, 64)        0         \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (None, 59, 59, 64)        36928     \n",
            "                                                                 \n",
            " prelu3 (PReLU)              (None, 59, 59, 64)        222784    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 29, 29, 64)        0         \n",
            "                                                                 \n",
            " conv4 (Conv2D)              (None, 28, 28, 128)       32896     \n",
            "                                                                 \n",
            " prelu4 (PReLU)              (None, 28, 28, 128)       100352    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 100352)            0         \n",
            "                                                                 \n",
            " conv5 (Dense)               (None, 256)               25690368  \n",
            "                                                                 \n",
            " prelu5 (PReLU)              (None, 256)               256       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " landmarks (Dense)           (None, 136)               34952     \n",
            "                                                                 \n",
            " prob1 (Softmax)             (None, 136)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29186504 (111.34 MB)\n",
            "Trainable params: 29186504 (111.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-203bec37e50d>:157: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = landmark_detector_model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 84s 4s/step - loss: 30483.2031 - prob1_loss: 19753.0840 - landmarks_loss: 10730.1211 - prob1_accuracy: 0.0000e+00 - landmarks_accuracy: 0.0000e+00 - val_loss: 21385.2754 - val_prob1_loss: 19939.6035 - val_landmarks_loss: 1445.6707 - val_prob1_accuracy: 0.0000e+00 - val_landmarks_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 59s 4s/step - loss: 22844.2559 - prob1_loss: 19752.8613 - landmarks_loss: 3091.3960 - prob1_accuracy: 0.0474 - landmarks_accuracy: 0.0474 - val_loss: 20335.5898 - val_prob1_loss: 19939.5547 - val_landmarks_loss: 396.0378 - val_prob1_accuracy: 0.1034 - val_landmarks_accuracy: 0.1034\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 59s 4s/step - loss: 21439.3477 - prob1_loss: 19752.7363 - landmarks_loss: 1686.6095 - prob1_accuracy: 0.0603 - landmarks_accuracy: 0.0603 - val_loss: 20297.6660 - val_prob1_loss: 19939.5156 - val_landmarks_loss: 358.1509 - val_prob1_accuracy: 0.4310 - val_landmarks_accuracy: 0.4310\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 65s 4s/step - loss: 20903.9355 - prob1_loss: 19752.6875 - landmarks_loss: 1151.2489 - prob1_accuracy: 0.0862 - landmarks_accuracy: 0.0862 - val_loss: 20600.1699 - val_prob1_loss: 19939.5039 - val_landmarks_loss: 660.6651 - val_prob1_accuracy: 0.4310 - val_landmarks_accuracy: 0.4310\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 59s 4s/step - loss: 20685.4492 - prob1_loss: 19752.6230 - landmarks_loss: 932.8235 - prob1_accuracy: 0.0948 - landmarks_accuracy: 0.0948 - val_loss: 20337.9570 - val_prob1_loss: 19939.5684 - val_landmarks_loss: 398.3888 - val_prob1_accuracy: 0.3103 - val_landmarks_accuracy: 0.3103\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 60s 4s/step - loss: 20457.2441 - prob1_loss: 19752.5840 - landmarks_loss: 704.6600 - prob1_accuracy: 0.1164 - landmarks_accuracy: 0.1164 - val_loss: 20193.7246 - val_prob1_loss: 19939.5039 - val_landmarks_loss: 254.2213 - val_prob1_accuracy: 0.4310 - val_landmarks_accuracy: 0.4310\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 60s 4s/step - loss: 20297.4434 - prob1_loss: 19752.5703 - landmarks_loss: 544.8741 - prob1_accuracy: 0.1336 - landmarks_accuracy: 0.1336 - val_loss: 20108.2832 - val_prob1_loss: 19939.5234 - val_landmarks_loss: 168.7598 - val_prob1_accuracy: 0.1552 - val_landmarks_accuracy: 0.1552\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 61s 4s/step - loss: 20293.1602 - prob1_loss: 19752.5664 - landmarks_loss: 540.5931 - prob1_accuracy: 0.1121 - landmarks_accuracy: 0.1121 - val_loss: 20152.1055 - val_prob1_loss: 19939.5176 - val_landmarks_loss: 212.5883 - val_prob1_accuracy: 0.2069 - val_landmarks_accuracy: 0.2069\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 59s 4s/step - loss: 20329.2637 - prob1_loss: 19752.5645 - landmarks_loss: 576.7004 - prob1_accuracy: 0.1250 - landmarks_accuracy: 0.1250 - val_loss: 20063.4707 - val_prob1_loss: 19939.5469 - val_landmarks_loss: 123.9214 - val_prob1_accuracy: 0.1034 - val_landmarks_accuracy: 0.1034\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 60s 4s/step - loss: 20266.9551 - prob1_loss: 19752.5781 - landmarks_loss: 514.3780 - prob1_accuracy: 0.1121 - landmarks_accuracy: 0.1121 - val_loss: 20102.9648 - val_prob1_loss: 19939.5469 - val_landmarks_loss: 163.4172 - val_prob1_accuracy: 0.3103 - val_landmarks_accuracy: 0.3103\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 59s 4s/step - loss: 20244.5879 - prob1_loss: 19752.5508 - landmarks_loss: 492.0387 - prob1_accuracy: 0.1552 - landmarks_accuracy: 0.1552 - val_loss: 20221.2578 - val_prob1_loss: 19939.5059 - val_landmarks_loss: 281.7527 - val_prob1_accuracy: 0.3621 - val_landmarks_accuracy: 0.3621\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 61s 4s/step - loss: 20182.0957 - prob1_loss: 19752.5508 - landmarks_loss: 429.5429 - prob1_accuracy: 0.1379 - landmarks_accuracy: 0.1379 - val_loss: 20062.9824 - val_prob1_loss: 19939.5234 - val_landmarks_loss: 123.4585 - val_prob1_accuracy: 0.3793 - val_landmarks_accuracy: 0.3793\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 60s 4s/step - loss: 20126.5781 - prob1_loss: 19752.5488 - landmarks_loss: 374.0293 - prob1_accuracy: 0.1379 - landmarks_accuracy: 0.1379 - val_loss: 20066.0840 - val_prob1_loss: 19939.5059 - val_landmarks_loss: 126.5768 - val_prob1_accuracy: 0.4138 - val_landmarks_accuracy: 0.4138\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 60s 4s/step - loss: 20118.0469 - prob1_loss: 19752.5391 - landmarks_loss: 365.5096 - prob1_accuracy: 0.1509 - landmarks_accuracy: 0.1509 - val_loss: 20064.1387 - val_prob1_loss: 19939.5078 - val_landmarks_loss: 124.6283 - val_prob1_accuracy: 0.4310 - val_landmarks_accuracy: 0.4310\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 59s 4s/step - loss: 20080.6465 - prob1_loss: 19752.5566 - landmarks_loss: 328.0909 - prob1_accuracy: 0.1293 - landmarks_accuracy: 0.1293 - val_loss: 20203.1074 - val_prob1_loss: 19939.5039 - val_landmarks_loss: 263.6051 - val_prob1_accuracy: 0.4310 - val_landmarks_accuracy: 0.4310\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 64s 4s/step - loss: 20078.7441 - prob1_loss: 19752.5391 - landmarks_loss: 326.2054 - prob1_accuracy: 0.1681 - landmarks_accuracy: 0.1681 - val_loss: 20022.5801 - val_prob1_loss: 19939.5176 - val_landmarks_loss: 83.0628 - val_prob1_accuracy: 0.1552 - val_landmarks_accuracy: 0.1552\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 61s 4s/step - loss: 20072.4590 - prob1_loss: 19752.5371 - landmarks_loss: 319.9238 - prob1_accuracy: 0.1724 - landmarks_accuracy: 0.1724 - val_loss: 20030.9570 - val_prob1_loss: 19939.5137 - val_landmarks_loss: 91.4451 - val_prob1_accuracy: 0.2414 - val_landmarks_accuracy: 0.2414\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 60s 4s/step - loss: 20068.5918 - prob1_loss: 19752.5430 - landmarks_loss: 316.0504 - prob1_accuracy: 0.1250 - landmarks_accuracy: 0.1250 - val_loss: 20030.0898 - val_prob1_loss: 19939.5137 - val_landmarks_loss: 90.5760 - val_prob1_accuracy: 0.1897 - val_landmarks_accuracy: 0.1897\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 61s 4s/step - loss: 20014.5430 - prob1_loss: 19752.5391 - landmarks_loss: 262.0009 - prob1_accuracy: 0.1638 - landmarks_accuracy: 0.1638 - val_loss: 20012.8184 - val_prob1_loss: 19939.5137 - val_landmarks_loss: 73.3080 - val_prob1_accuracy: 0.4310 - val_landmarks_accuracy: 0.4310\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 62s 4s/step - loss: 20034.5254 - prob1_loss: 19752.5410 - landmarks_loss: 281.9804 - prob1_accuracy: 0.1853 - landmarks_accuracy: 0.1853 - val_loss: 20080.8789 - val_prob1_loss: 19939.5059 - val_landmarks_loss: 141.3730 - val_prob1_accuracy: 0.4138 - val_landmarks_accuracy: 0.4138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-203bec37e50d>:166: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  loss = landmark_detector_model.evaluate_generator(val_data_generator, steps=validation_steps)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: [20063.111328125, 19939.505859375, 123.60707092285156, 0.43103447556495667, 0.43103447556495667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess_image(img):\n",
        "    # Resize the image to the input size expected by your model\n",
        "    target_height, target_width = 256, 256  # Adjust based on your model's input size\n",
        "    img_resized = cv2.resize(img, (target_width, target_height))\n",
        "\n",
        "    # Normalize pixel values to the range [0, 1]\n",
        "    img_normalized = img_resized / 255.0\n",
        "\n",
        "    return img_normalized\n",
        "# Function to draw landmarks on the image using Matplotlib\n",
        "def draw_landmarks_matplotlib(img, landmarks, save_path=None):\n",
        "    # Assuming landmarks is a list with the second element containing coordinates\n",
        "    landmark_coordinates = landmarks[1][0]\n",
        "\n",
        "    # Reshape the flat array into pairs of (x, y) coordinates\n",
        "    landmark_coordinates = landmark_coordinates.reshape((-1, 2))\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Draw circles at each landmark position\n",
        "    for x, y in landmark_coordinates:\n",
        "        circle = plt.Circle((x, y), radius=2, color='green')\n",
        "        ax.add_patch(circle)\n",
        "\n",
        "    # Save or display the plot\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# Path to the single image you want to test\n",
        "image_path = '/content/WIN_20240125_12_57_04_Pro.jpg'\n",
        "\n",
        "# Read the image\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "# Preprocess the image\n",
        "img_preprocessed = preprocess_image(img)\n",
        "print(img_preprocessed.shape)\n",
        "# Predict landmarks using your model\n",
        "landmarks = landmark_detector_model.predict(np.expand_dims(img_preprocessed, axis=0))\n",
        "\n",
        "# Draw landmarks on the image using Matplotlib\n",
        "draw_landmarks_matplotlib(img_preprocessed, landmarks)\n"
      ],
      "metadata": {
        "id": "02w4b5C47vgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "bVrb9UssS2R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "metadata": {
        "id": "_9eAnbDsS7Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "DV7b6GOVS_rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess_image(img):\n",
        "    # Resize the image to the input size expected by your model\n",
        "    target_height, target_width = 256, 256  # Adjust based on your model's input size\n",
        "    img_resized = cv2.resize(img, (target_width, target_height))\n",
        "\n",
        "    # Normalize pixel values to the range [0, 1]\n",
        "    img_normalized = img_resized / 255.0\n",
        "\n",
        "    return img_normalized\n",
        "# Function to draw landmarks on the image using Matplotlib\n",
        "def draw_landmarks_matplotlib(img, landmarks, save_path=None):\n",
        "    # Assuming landmarks is a list with the second element containing coordinates\n",
        "    landmark_coordinates = landmarks[1][0]\n",
        "\n",
        "    # Reshape the flat array into pairs of (x, y) coordinates\n",
        "    landmark_coordinates = landmark_coordinates.reshape((-1, 2))\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Draw circles at each landmark position\n",
        "    for x, y in landmark_coordinates:\n",
        "        circle = plt.Circle((x, y), radius=2, color='green')\n",
        "        ax.add_patch(circle)\n",
        "\n",
        "    # Save or display the plot\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# Path to the single image you want to test\n",
        "image_path = '/content/WIN_20240125_12_57_04_Pro.jpg'\n",
        "\n",
        "# Read the image\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "# Preprocess the image\n",
        "img_preprocessed = preprocess_image(img)\n",
        "# print(img_preprocessed.shape)\n",
        "# Predict landmarks using your model\n",
        "landmarks = landmark_detector_model.predict(np.expand_dims(img_preprocessed, axis=0))\n",
        "\n",
        "# Draw landmarks on the image using Matplotlib\n",
        "draw_landmarks_matplotlib(img_preprocessed, landmarks)\n"
      ],
      "metadata": {
        "id": "ROQfmsGn-ICY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialize bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "\n",
        "    img_preprocessed = preprocess_image(img)\n",
        "# print(img_preprocessed.shape)\n",
        "# Predict landmarks using your model\n",
        "    landmarks = landmark_detector_model.predict(np.expand_dims(img_preprocessed, axis=0))\n",
        "\n",
        "# Draw landmarks on the image using Matplotlib\n",
        "    draw_landmarks_matplotlib(img_preprocessed, landmarks)\n",
        "\n",
        "\n",
        "    # save the image with landmarks\n",
        "    cv2.imwrite(f'frame_{count}.png', cv2.cvtColor((img * 255.0).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    count += 1\n"
      ],
      "metadata": {
        "id": "JT4QOJ79UftS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}